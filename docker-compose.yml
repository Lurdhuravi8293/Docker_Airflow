services:
  airflow:
    image: apache/airflow:2.7.2
    container_name: airflow
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./dags:/opt/airflow/dags
      - ./etl_pipeline:/opt/etl           # ðŸ‘ˆ or spark_jobs if thatâ€™s your folder
    ports:
      - "8082:8082"                       # âœ… You can keep this at 8080 for standard UI
    entrypoint: >                         # âœ… Replaces 'command'
      bash -c "
        airflow db init &&
        airflow users create --username airflow --password airflow --firstname admin --lastname user --role Admin --email admin@example.com &&
        airflow scheduler &
        exec airflow webserver
      "
    depends_on:
      - spark

  spark:
    image: bitnami/spark:latest
    container_name: spark
    ports:
      - "4042:4042"                       # âœ… Changed back to default Spark UI port
